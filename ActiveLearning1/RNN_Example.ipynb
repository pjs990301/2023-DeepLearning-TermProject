{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "id": "Og4ASBkUd4eY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "z-Z39U1iV9wW"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class myRNN(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, output_size):\n",
        "    super(myRNN, self).__init__()\n",
        "\n",
        "    self.input_size = input_size # length of one-hot encoding vector\n",
        "    self.hidden_size = hidden_size # hidden state size which is determined by a designer\n",
        "    self.output_size = output_size # length of on-hot encoding vector\n",
        "   \n",
        "    # U, W, V are implemented by fully-connected layers\n",
        "    self.U = nn.Linear(input_size, hidden_size)\n",
        "    self.W = nn.Linear(hidden_size, hidden_size)\n",
        "    self.V = nn.Linear(hidden_size, output_size)\n",
        "    \n",
        "    self.activation_func = nn.Tanh() # activation cuntion = tanh \n",
        "    self.softmax = nn.LogSoftmax(dim=0) # Softmax function contains Cross-Entropy Loss\n",
        "\n",
        "  def forward(self, input, hidden):\n",
        "    hidden = self.activation_func(self.U(input) + self.W(hidden)) # ht = tanh(U*xt + W*ht+1)\n",
        "    output = self.V(hidden) # softmax(V*ht)\n",
        "    return output, hidden\n",
        "    \n",
        "  def init_hidden(self):\n",
        "    return torch.zeros(1, self.hidden_size) # initialization of the first hidden values "
      ],
      "metadata": {
        "id": "ZfkCGlQ7WJMI"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define character list as the string type \n",
        "chars = \"abcdefghijklmnopqrstuvwxyz .,:;?01\" # index 0123456 .......\n",
        "\n",
        "# convert character list to list type\n",
        "char_list = [i for i in chars] \n",
        "\n",
        "# get the length of character list\n",
        "n_letters = len(char_list)"
      ],
      "metadata": {
        "id": "vi_6H3CtWUXT"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For Input \n",
        "def word_to_onehot(string):\n",
        "  # we will use start and end code by using 0, 1\n",
        "  start = np.zeros(shape=len(char_list), dtype=int)\n",
        "  end = np.zeros(shape=len(char_list), dtype=int)\n",
        "  start[-2] = 1\n",
        "  end[-1] = 1\n",
        "  \n",
        "  for i in string:\n",
        "  # one-hot encoding of training string will be located between start and end vector\n",
        "    idx = char_list.index(i)\n",
        "    zero = np.zeros(shape=n_letters, dtype=int)\n",
        "    zero[idx] = 1\n",
        "    start = np.vstack([start, zero])\n",
        "  output = np.vstack([start, end])\n",
        "  return output"
      ],
      "metadata": {
        "id": "qey-V262dNOx"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For Output / Test \n",
        "def onehot_to_word(onehot_1): \n",
        "  onehot = torch.Tensor.numpy(onehot_1)\n",
        "  return char_list[onehot.argmax()]"
      ],
      "metadata": {
        "id": "csWneOtLdoJ7"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "  n_hidden = 128 # set hidden size 128 \n",
        "  lr = 0.001 # set learning rate to 0.001 \n",
        "  epochs = 1000 # set epochs to 1000 (5000)\n",
        "  \n",
        "  # set training string\n",
        "  # < Question 1 > \n",
        "  string = \"i want to go on a trip these days. how about you?\"\n",
        "  \n",
        "  # < Question 2 > \n",
        "  #string = \"i want to go on a trip these days. how about you? i would like to visit france, italy, germany again with my friends.\"\n",
        "  \n",
        "  # initiate RNN\n",
        "  rnn = myRNN(n_letters, n_hidden, n_letters)\n",
        "   \n",
        "  loss_func = nn.CrossEntropyLoss()  # use Cross-Entropy Loss as loss function \n",
        "  optimizer = torch.optim.Adam(rnn.parameters(), lr=lr) # Adam optimizer \n",
        "  \n",
        "  ## Training \n",
        "  one_hot = torch.from_numpy(word_to_onehot(string)).type_as(torch.FloatTensor())\n",
        "  \n",
        "  for i in range(epochs):\n",
        "    optimizer.zero_grad()\n",
        "    total_loss = 0\n",
        "    hidden = rnn.init_hidden()\n",
        "    \n",
        "    input_ = one_hot[0:1, :]\n",
        "    \n",
        "    for j in range(one_hot.size()[0]-1):\n",
        "      target = one_hot[j+1]\n",
        "      target_single = torch.from_numpy(np.asarray(target.numpy().argmax())).type_as(torch.LongTensor()).view(-1)\n",
        "      \n",
        "      output, hidden = rnn.forward(input_, hidden)\n",
        "      loss = loss_func(output, target_single)\n",
        "      total_loss += loss\n",
        "      input_ = output\n",
        "    \n",
        "    total_loss.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "    if i%100 == 0:\n",
        "      print('epoch%d'%i)\n",
        "      print(total_loss)\n",
        "       \n",
        "  ## Test\n",
        "  start = torch.zeros(1, len(char_list))\n",
        "  start[:,-2] = 1\n",
        "  \n",
        "  with torch.no_grad():\n",
        "    hidden = rnn.init_hidden()\n",
        "    input_ = start\n",
        "    output_string = \"\"\n",
        "    \n",
        "    for i in range(len(string)):\n",
        "      output, hidden = rnn.forward(input_, hidden)\n",
        "      output_string += onehot_to_word(F.softmax(output.data))\n",
        "      input_ = output\n",
        "      \n",
        "    print(output_string)\n"
      ],
      "metadata": {
        "id": "vRbWmhz2e6nb"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run the code"
      ],
      "metadata": {
        "id": "mWCWHwgPrA7k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0snEiNJKn9RI",
        "outputId": "fa62977a-2b0e-4376-8d05-cebfcc1b799f"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch0\n",
            "tensor(178.1885, grad_fn=<AddBackward0>)\n",
            "epoch100\n",
            "tensor(117.1459, grad_fn=<AddBackward0>)\n",
            "epoch200\n",
            "tensor(99.0269, grad_fn=<AddBackward0>)\n",
            "epoch300\n",
            "tensor(61.3565, grad_fn=<AddBackward0>)\n",
            "epoch400\n",
            "tensor(57.3816, grad_fn=<AddBackward0>)\n",
            "epoch500\n",
            "tensor(48.0377, grad_fn=<AddBackward0>)\n",
            "epoch600\n",
            "tensor(39.2804, grad_fn=<AddBackward0>)\n",
            "epoch700\n",
            "tensor(32.1643, grad_fn=<AddBackward0>)\n",
            "epoch800\n",
            "tensor(14.9323, grad_fn=<AddBackward0>)\n",
            "epoch900\n",
            "tensor(4.8947, grad_fn=<AddBackward0>)\n",
            "i want to go on a trip these days. how about you?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-77-907484017bda>:56: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output_string += onehot_to_word(F.softmax(output.data))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Result Analysis\n",
        "\n",
        "GT = i want to go on a trip theses days. how about you?\n",
        "\n",
        "How can we improve the performace ? \n"
      ],
      "metadata": {
        "id": "f-eP0j3_i76j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Try many other options"
      ],
      "metadata": {
        "id": "JATUfqrjqtdn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# increase number of hidden layer \n",
        "n_hidden = 256 \n",
        "\n",
        "# increase epochs\n",
        "epochs = 5000 \n",
        "\n",
        "# etc... "
      ],
      "metadata": {
        "id": "HB575qghqd0g"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}